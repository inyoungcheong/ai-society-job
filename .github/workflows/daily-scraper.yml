name: Daily Job Scraping

on:
  # Run daily at 2 AM UTC
  schedule:
    - cron: '0 2 * * *'
  
  # Allow manual execution
  workflow_dispatch:
    inputs:
      debug_mode:
        description: 'Debug mode (verbose logging)'
        required: false
        default: 'false'
        type: boolean

# Required permissions
permissions:
  contents: write  # Required for file modifications

jobs:
  scrape-jobs:
    runs-on: ubuntu-latest
    
    steps:
    # 1. Checkout repository
    - name: ðŸ“¥ Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    # 2. Set up Python environment
    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    # 3. Install dependencies
    - name: ðŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    # 4. Run job scraper
    - name: ðŸ” Run job scraper
      env:
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        DEBUG_MODE: ${{ github.event.inputs.debug_mode }}
      run: |
        echo "ðŸš€ Starting job scraping..."
        python scraper.py
        
        # Verify results
        if [ -f "data/jobs.json" ]; then
          echo "âœ… jobs.json created successfully"
          echo "ðŸ“Š File size: $(du -h data/jobs.json | cut -f1)"
          echo "ðŸ”¢ Job count: $(python -c "import json; print(len(json.load(open('data/jobs.json'))['jobs']))")"
        else
          echo "âŒ jobs.json not found!"
          exit 1
        fi

    # 5. Check for changes in data files
    - name: ðŸ“‹ Check for changes
      id: check_changes
      run: |
        if git diff --quiet data/; then
          echo "changes=false" >> $GITHUB_OUTPUT
          echo "ðŸ“ No changes detected in data files"
        else
          echo "changes=true" >> $GITHUB_OUTPUT
          echo "ðŸ“ Changes detected in data files"
          git diff --name-only data/
        fi

    # 6. Configure Git
    - name: âš™ï¸ Configure Git
      if: steps.check_changes.outputs.changes == 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action Bot"

    # 7. Commit and push changes
    - name: ðŸ’¾ Commit and push changes
      if: steps.check_changes.outputs.changes == 'true'
      run: |
        # Get current time (Seoul timezone)
        CURRENT_TIME=$(TZ='Asia/Seoul' date '+%Y-%m-%d %H:%M:%S KST')
        
        # Get statistics
        TOTAL_JOBS=$(python -c "import json; print(json.load(open('data/jobs.json'))['stats']['total'])" 2>/dev/null || echo "0")
        NEW_TODAY=$(python -c "import json; print(json.load(open('data/jobs.json'))['stats']['new_today'])" 2>/dev/null || echo "0")
        
        # Create commit message
        COMMIT_MSG="ðŸ¤– Update job data - $CURRENT_TIME

        ðŸ“Š Total jobs: $TOTAL_JOBS
        ðŸ†• New today: $NEW_TODAY
        
        Auto-generated by GitHub Actions"
        
        # Add files and commit
        git add data/
        git commit -m "$COMMIT_MSG"
        git push

    # 8. Job summary
    - name: ðŸ“ˆ Job Summary
      run: |
        echo "## ðŸŽ¯ Scraping Results Summary" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "data/jobs.json" ]; then
          TOTAL=$(python -c "import json; print(json.load(open('data/jobs.json'))['stats']['total'])")
          FACULTY=$(python -c "import json; print(json.load(open('data/jobs.json'))['stats']['faculty'])")
          INDUSTRY=$(python -c "import json; print(json.load(open('data/jobs.json'))['stats']['industry'])")
          NONPROFIT=$(python -c "import json; print(json.load(open('data/jobs.json'))['stats']['nonprofit'])")
          NEW_TODAY=$(python -c "import json; print(json.load(open('data/jobs.json'))['stats']['new_today'])")
          
          echo "| Category | Count |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ“Š Total Jobs | $TOTAL |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸŽ“ Faculty | $FACULTY |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ¢ Industry | $INDUSTRY |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ›ï¸ Non-profit | $NONPROFIT |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ†• New Today | $NEW_TODAY |" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ•’ **Last Updated:** $(TZ='Asia/Seoul' date '+%Y-%m-%d %H:%M:%S KST')" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Error:** jobs.json file not found." >> $GITHUB_STEP_SUMMARY
        fi

    # 9. Notify on failure
    - name: ðŸš¨ Notify on failure
      if: failure()
      run: |
        echo "âŒ Job scraping failed!" >> $GITHUB_STEP_SUMMARY
        echo "Please check the logs for details." >> $GITHUB_STEP_SUMMARY
        
        # Upload error log if exists
        if [ -f "scraper_error.log" ]; then
          echo "Error log:" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat scraper_error.log >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
        fi

  # Additional job: Check website deployment
  check-deployment:
    runs-on: ubuntu-latest
    needs: scrape-jobs
    if: success()
    
    steps:
    - name: ðŸŒ Check website deployment
      run: |
        # Wait for GitHub Pages deployment
        sleep 30
        
        # Check website response
        REPO_NAME="${{ github.repository }}"
        USERNAME="${{ github.repository_owner }}"
        SITE_URL="https://${USERNAME}.github.io/${REPO_NAME##*/}"
        
        echo "ðŸ” Checking website: $SITE_URL"
        
        if curl -f -s "$SITE_URL" > /dev/null; then
          echo "âœ… Website is accessible: $SITE_URL"
          echo "ðŸŽ‰ **Website updated successfully!**" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ”— **Site URL:** $SITE_URL" >> $GITHUB_STEP_SUMMARY
        else
          echo "âš ï¸ Website might not be ready yet: $SITE_URL"
          echo "â³ GitHub Pages deployment may take a few minutes to complete." >> $GITHUB_STEP_SUMMARY
        fi
