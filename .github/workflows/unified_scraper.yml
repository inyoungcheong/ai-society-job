name: AI & Society Job Scraper - Unified

on:
  schedule:
    # Run twice a week: Tuesday and Friday at 10:00 UTC (19:00 KST)
    - cron: '0 10 * * 2,5'
  workflow_dispatch:  # Allow manual trigger
  push:
    paths:
      - 'ac_scraper.py'
      - 'j_scraper.py' 
      - 'li_scraper.py'
      - '.github/workflows/unified-scraper.yml'

# Explicit permissions for repository access
permissions:
  contents: write
  actions: read

jobs:
  unified-job-scraping:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Generous timeout for all scrapers
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Set up Node.js (for LinkedIn scraper)
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests google-generativeai python-dotenv
        
    - name: Install Node.js dependencies (if needed)
      run: |
        if [ -f "package.json" ]; then
          npm install
        else
          npm init -y
          npm install linkedin-jobs-api
        fi
        
    - name: Create data directory
      run: |
        mkdir -p data
        echo "ğŸ“ Data directory created"
        
    # Phase 1: Academic Jobs (Fastest, no API limits)
    - name: Run Academic Jobs Scraper
      env:
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      run: |
        echo "ğŸ“ Starting Academic Jobs scraping..."
        python ac_scraper.py || echo "âš ï¸ Academic scraper had issues, continuing..."
        
        if [ -f "data/ajo_gemini_jobs.json" ]; then
          echo "âœ… Academic jobs data created"
          echo "ğŸ“Š Size: $(du -h data/ajo_gemini_jobs.json | cut -f1)"
        else
          echo "âš ï¸ No academic jobs data generated"
        fi
        
    # Phase 2: JSearch API (Limited to 25 calls)  
    - name: Run JSearch + Gemini Scraper
      env:
        RAPIDAPI_KEY: ${{ secrets.RAPIDAPI_KEY }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      run: |
        echo "ğŸ” Starting JSearch + Gemini scraping..."
        python j_scraper.py || echo "âš ï¸ JSearch scraper had issues, continuing..."
        
        if [ -f "data/jsearch_gemini_jobs.json" ]; then
          echo "âœ… JSearch jobs data created"
          echo "ğŸ“Š Size: $(du -h data/jsearch_gemini_jobs.json | cut -f1)"
          echo "ğŸ”¢ Job count: $(python -c "import json; data=json.load(open('data/jsearch_gemini_jobs.json')); print(data['metadata']['total_jobs'])" 2>/dev/null || echo 'Unknown')"
          echo "ğŸ“ API calls: $(python -c "import json; data=json.load(open('data/jsearch_gemini_jobs.json')); print(data['metadata']['api_calls_used'])" 2>/dev/null || echo 'Unknown')"
        else
          echo "âš ï¸ No JSearch jobs data generated"
        fi
        
    # Phase 3: LinkedIn Jobs (Most comprehensive)
    - name: Run LinkedIn + Gemini Scraper
      env:
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      run: |
        echo "ğŸ’¼ Starting LinkedIn + Gemini scraping..."
        python li_scraper.py || echo "âš ï¸ LinkedIn scraper setup had issues, continuing..."
        
        # Check if Node.js scraper was created and run it
        if [ -f "linkedin_scraper.js" ]; then
          echo "ğŸš€ Running LinkedIn Node.js scraper..."
          timeout 20m npm run scrape || echo "âš ï¸ LinkedIn scraping timed out or had issues"
        fi
        
        if [ -f "data/linkedin_gemini_jobs.json" ]; then
          echo "âœ… LinkedIn jobs data created"
          echo "ğŸ“Š Size: $(du -h data/linkedin_gemini_jobs.json | cut -f1)"
        else
          echo "âš ï¸ No LinkedIn jobs data generated"
        fi
        
    # Phase 4: Data Integration and Summary
    - name: Integrate and analyze all data
      run: |
        echo "ğŸ”„ Integrating all job data sources..."
        
        cat > integrate_data.py << 'EOF'
        import json
        import os
        from datetime import datetime

        def load_json_safely(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {'jobs': [], 'metadata': {}}

        # Load all data sources
        academic_data = load_json_safely('data/ajo_gemini_jobs.json')
        jsearch_data = load_json_safely('data/jsearch_gemini_jobs.json') 
        linkedin_data = load_json_safely('data/linkedin_gemini_jobs.json')

        # Combine all jobs
        all_jobs = []
        all_jobs.extend(academic_data.get('jobs', []))
        all_jobs.extend(jsearch_data.get('jobs', []))
        all_jobs.extend(linkedin_data.get('jobs', []))

        # Remove duplicates based on URL and title+company
        seen = set()
        unique_jobs = []

        for job in all_jobs:
            # Create unique identifier
            url = job.get('source_url', '')
            title_company = f"{job.get('title', '')}-{job.get('company', '')}"
            identifier = url if url else title_company
            
            if identifier not in seen:
                seen.add(identifier)
                unique_jobs.append(job)

        # Sort by relevance score
        unique_jobs.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)

        # Calculate combined statistics
        stats = {
            'total_jobs': len(unique_jobs),
            'academic_jobs': len(academic_data.get('jobs', [])),
            'jsearch_jobs': len(jsearch_data.get('jobs', [])), 
            'linkedin_jobs': len(linkedin_data.get('jobs', [])),
            'duplicates_removed': len(all_jobs) - len(unique_jobs),
            'high_relevance': len([j for j in unique_jobs if j.get('relevance_score', 0) >= 80]),
            'gemini_analyzed': len([j for j in unique_jobs if j.get('gemini_analyzed', False)]),
            'remote_jobs': len([j for j in unique_jobs if j.get('is_remote', False)]),
            'by_job_type': {},
            'by_category': {},
            'sources': {
                'academic': len(academic_data.get('jobs', [])),
                'jsearch': len(jsearch_data.get('jobs', [])),
                'linkedin': len(linkedin_data.get('jobs', []))
            }
        }

        # Count by job type and category
        for job in unique_jobs:
            job_type = job.get('job_type', 'unknown')
            stats['by_job_type'][job_type] = stats['by_job_type'].get(job_type, 0) + 1
            
            category = job.get('category', 'unknown')  
            stats['by_category'][category] = stats['by_category'].get(category, 0) + 1

        # Create master data file
        master_data = {
            'jobs': unique_jobs,
            'stats': stats,
            'metadata': {
                'last_update': datetime.now().isoformat(),
                'total_unique_jobs': len(unique_jobs),
                'sources_integrated': ['academic', 'jsearch', 'linkedin'],
                'integration_date': datetime.now().isoformat()
            }
        }

        # Save master file
        with open('data/all_jobs_integrated.json', 'w', encoding='utf-8') as f:
            json.dump(master_data, f, ensure_ascii=False, indent=2)

        # Create summary
        print(f'ğŸ¯ Data Integration Complete!')
        print(f'   Academic jobs: {stats["academic_jobs"]}')
        print(f'   JSearch jobs: {stats["jsearch_jobs"]}')
        print(f'   LinkedIn jobs: {stats["linkedin_jobs"]}')
        print(f'   Total unique: {stats["total_jobs"]}')
        print(f'   Duplicates removed: {stats["duplicates_removed"]}')
        print(f'   High relevance (80+): {stats["high_relevance"]}')
        print(f'   Gemini analyzed: {stats["gemini_analyzed"]}')
        print(f'   Remote jobs: {stats["remote_jobs"]}')

        print(f'\nğŸ“‚ By Job Type:')
        for job_type, count in stats['by_job_type'].items():
            print(f'   {job_type}: {count}')
            
        print(f'\nğŸ“‚ By Category:')
        for category, count in stats['by_category'].items():
            print(f'   {category}: {count}')
        EOF
        
        python integrate_data.py
        
    # Phase 5: Update legacy files for HTML compatibility
    - name: Update legacy data files
      run: |
        echo "ğŸ”„ Updating legacy data files for HTML compatibility..."
        
        # Copy master file to jobs.json for HTML compatibility
        if [ -f "data/all_jobs_integrated.json" ]; then
          cp data/all_jobs_integrated.json data/jobs.json
          echo "âœ… Updated data/jobs.json for HTML"
        fi
        
        # Ensure jsearch_jobs.json exists for HTML
        if [ ! -f "data/jsearch_jobs.json" ] && [ -f "data/jsearch_gemini_jobs.json" ]; then
          cp data/jsearch_gemini_jobs.json data/jsearch_jobs.json
          echo "âœ… Created data/jsearch_jobs.json compatibility file"
        fi
        
        # Create stats summary
        cat > create_summary.py << 'EOF'
        import json
        from datetime import datetime

        try:
            with open('data/all_jobs_integrated.json', 'r') as f:
                data = json.load(f)
            
            summary = {
                'last_update': data['metadata']['last_update'],
                'total_jobs': data['stats']['total_jobs'],
                'sources': data['stats']['sources'],
                'quality': {
                    'high_relevance': data['stats']['high_relevance'],
                    'gemini_analyzed': data['stats']['gemini_analyzed']
                }
            }
            
            with open('data/scraping_summary.json', 'w') as f:
                json.dump(summary, f, indent=2)
                
            print('âœ… Created scraping summary')
        except Exception as e:
            print(f'âš ï¸ Could not create summary: {e}')
        EOF
        
        python create_summary.py
        
    # Phase 6: Git commit and push
    - name: Commit and push all results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "AI Society Job Scraper"
        
        # Check for unstaged changes and stash if needed
        if ! git diff --quiet; then
          echo "Stashing unstaged changes..."
          git stash
        fi
        
        # Pull latest changes to avoid conflicts
        git pull origin main --rebase || echo "No remote changes to pull"
        
        # Pop stash if we stashed changes
        if git stash list | grep -q "stash@{0}"; then
          git stash pop || echo "Could not pop stash"
        fi
        
        # Add all data files
        git add data/ || echo "No data files to add"
        
        # Add any generated scraper files
        git add *.js package.json *.py 2>/dev/null || echo "No additional files to add"
        
        # Check if there are changes to commit
        if ! git diff --staged --quiet; then
          # Create detailed commit message
          echo "ğŸ“Š AI & Society Jobs Update - $(date '+%Y-%m-%d %H:%M UTC')" > commit_msg.tmp
          echo "" >> commit_msg.tmp
          echo "ğŸ¯ Integrated job data from multiple sources:" >> commit_msg.tmp
          echo "- Academic jobs (Gemini enhanced)" >> commit_msg.tmp
          echo "- JSearch API (Google Jobs + Gemini)" >> commit_msg.tmp  
          echo "- LinkedIn API (Gemini filtered)" >> commit_msg.tmp
          echo "" >> commit_msg.tmp
          
          # Add job counts if available
          if [ -f "data/all_jobs_integrated.json" ]; then
            python3 -c "
import json
try:
    with open('data/all_jobs_integrated.json', 'r') as f:
        data = json.load(f)
    stats = data['stats']
    print('ğŸ“ˆ Results: {} unique jobs'.format(stats.get('total_jobs', stats.get('total', 0))))
    print('   Academic: {}'.format(stats.get('sources', {}).get('academic', 0)))
    print('   JSearch: {}'.format(stats.get('sources', {}).get('jsearch', 0)))
    print('   LinkedIn: {}'.format(stats.get('sources', {}).get('linkedin', 0)))
    print('   High relevance: {}'.format(stats.get('high_relevance', 0)))
except:
    print('ğŸ“ˆ Job data updated')
" >> commit_msg.tmp
          fi
          
          git commit -F commit_msg.tmp
          rm commit_msg.tmp
          
          # Push with retry logic
          for i in {1..3}; do
            if git push origin main; then
              echo "âœ… Successfully pushed to repository"
              break
            else
              echo "âš ï¸ Push attempt $i failed, retrying..."
              sleep 5
              git pull origin main --rebase || echo "Failed to rebase, continuing..."
            fi
          done
        else
          echo "â„¹ï¸ No changes to commit"
        fi
        
    # Phase 7: Final summary and cleanup
    - name: Final summary and cleanup
      run: |
        echo "ğŸ‰ AI & Society Job Scraping Workflow Complete!"
        echo "============================================================"
        
        # List all generated files
        echo "ğŸ“ Generated data files:"
        ls -la data/ 2>/dev/null || echo "No data directory found"
        
        # Show file sizes
        if [ -d "data" ]; then
          echo ""
          echo "ğŸ“Š File sizes:"
          du -h data/* 2>/dev/null || echo "No data files found"
        fi
        
        # Final status
        echo ""
        echo "âœ… Workflow completed successfully!"
        echo "ğŸ•’ Next run: Tuesday or Friday at 10:00 UTC"
        echo "ğŸŒ Data available at: https://github.com/${{ github.repository }}/tree/main/data"
        
        # Cleanup temporary files
        rm -f commit_msg.tmp linkedin_scraper.js linkedin_gemini_analyzer.py integrate_data.py create_summary.py get_stats.py 2>/dev/null || true
