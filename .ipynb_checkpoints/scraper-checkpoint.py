#!/usr/bin/env python3
"""
AI & Society Job Scraper
í¬ë¡¤ë§í•´ì„œ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import requests
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any
import time
import re
from urllib.parse import urljoin, urlparse

# API í‚¤ (GitHub Secretsì—ì„œ ê°€ì ¸ì˜¤ê¸°)
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

class AIJobScraper:
    def __init__(self):
        self.jobs = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; JobBot/1.0; +https://github.com/your-username/job-leaderboard)'
        })
        
        # AI & Society ê´€ë ¨ í‚¤ì›Œë“œ
        self.relevant_keywords = [
            'ai ethics', 'artificial intelligence', 'machine learning', 'ai policy',
            'algorithmic fairness', 'ai governance', 'technology law', 'digital rights',
            'ai safety', 'responsible ai', 'ai regulation', 'tech policy',
            'algorithmic accountability', 'ai transparency', 'digital policy',
            'technology ethics', 'computational social science', 'human-computer interaction'
        ]
        
        # ì œì™¸í•  í‚¤ì›Œë“œ (ë„ˆë¬´ ê¸°ìˆ ì ì´ê±°ë‚˜ ê´€ë ¨ ì—†ëŠ” ê²ƒë“¤)
        self.exclude_keywords = [
            'software engineer', 'data engineer', 'devops', 'backend developer',
            'frontend developer', 'full stack', 'mobile developer', 'qa engineer'
        ]

    def calculate_relevance_score(self, job_data: Dict) -> int:
        """Gemini APIë¥¼ ì‚¬ìš©í•´ì„œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        text = f"{job_data.get('title', '')} {job_data.get('description', '')}"
        text = text.lower()
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        relevance_score = 0
        
        # ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì ìˆ˜ ì¶”ê°€
        for keyword in self.relevant_keywords:
            if keyword in text:
                relevance_score += 10
                
        # ì œì™¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì ìˆ˜ ê°ì 
        for keyword in self.exclude_keywords:
            if keyword in text:
                relevance_score -= 15
                
        # Gemini APIë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ ì ìˆ˜ ê³„ì‚°
        if GEMINI_API_KEY:
            ai_score = self.get_gemini_relevance_score(text)
            relevance_score = max(relevance_score, ai_score)
            
        return max(0, min(100, relevance_score))

    def get_gemini_relevance_score(self, text: str) -> int:
        """Gemini APIë¥¼ ì‚¬ìš©í•´ì„œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            import google.generativeai as genai
            
            # API í‚¤ ì„¤ì •
            genai.configure(api_key=GEMINI_API_KEY)
            model = genai.GenerativeModel('gemini-1.5-flash')
            
            prompt = f"""
            ë‹¤ìŒ ì±„ìš©ê³µê³ ê°€ "AI & Society" ë¶„ì•¼ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€ 0-100ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
            
            AI & Society ë¶„ì•¼ëŠ” ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:
            - AI ìœ¤ë¦¬ (AI Ethics)
            - AI ì •ì±… ë° ê±°ë²„ë„ŒìŠ¤ (AI Policy & Governance)  
            - ê¸°ìˆ  ë²•ë¥  (Technology Law)
            - ì•Œê³ ë¦¬ì¦˜ ê³µì •ì„± (Algorithmic Fairness)
            - AI ì•ˆì „ì„± (AI Safety)
            - ë””ì§€í„¸ ê¶Œë¦¬ (Digital Rights)
            - ê¸°ìˆ ì˜ ì‚¬íšŒì  ì˜í–¥ (Social Impact of Technology)
            - ì±…ì„ìˆëŠ” AI (Responsible AI)
            
            ì±„ìš©ê³µê³  ë‚´ìš©:
            ì œëª©: {job_data.get('title', '')}
            ì„¤ëª…: {text[:800]}
            
            í‰ê°€ ê¸°ì¤€:
            - 90-100ì : í•µì‹¬ AI & Society ë¶„ì•¼ (AI ìœ¤ë¦¬ ì—°êµ¬ì›, AI ì •ì±… ë§¤ë‹ˆì € ë“±)
            - 70-89ì : ê°•í•œ ê´€ë ¨ì„± (AI ê´€ë ¨ ë²•ë¥ , ê¸°ìˆ  ì •ì±… ë“±)  
            - 50-69ì : ì¤‘ê°„ ê´€ë ¨ì„± (ì¼ë°˜ AI ì—°êµ¬ì—ì„œ ì‚¬íšŒì  ì¸¡ë©´ ê³ ë ¤)
            - 30-49ì : ì•½í•œ ê´€ë ¨ì„± (ê¸°ìˆ  ë¶„ì•¼ì´ì§€ë§Œ ì‚¬íšŒì  ì¸¡ë©´ ë¯¸ë¯¸)
            - 0-29ì : ê´€ë ¨ì„± ì—†ìŒ (ìˆœìˆ˜ ê¸°ìˆ  ê°œë°œ, ì—”ì§€ë‹ˆì–´ë§ ë“±)
            
            ì ìˆ˜ë§Œ ìˆ«ìë¡œ ë‹µí•´ì£¼ì„¸ìš” (0-100):
            """
            
            response = model.generate_content(prompt)
            score_text = response.text.strip()
            
            # ìˆ«ì ì¶”ì¶œ
            import re
            numbers = re.findall(r'\d+', score_text)
            if numbers:
                score = int(numbers[0])
                return max(0, min(100, score))
                
        except Exception as e:
            print(f"Gemini API ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            
        return 0

    def scrape_indeed(self, query: str = "AI ethics policy") -> List[Dict]:
        """Indeedì—ì„œ ì±„ìš©ê³µê³  í¬ë¡¤ë§"""
        jobs = []
        
        try:
            # Indeed ê²€ìƒ‰ URL
            url = f"https://www.indeed.com/jobs?q={query.replace(' ', '+')}&l=&sort=date"
            
            response = self.session.get(url, timeout=10)
            if response.status_code != 200:
                print(f"Indeed ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                return jobs
                
            # ê°„ë‹¨í•œ íŒŒì‹± (ì‹¤ì œë¡œëŠ” BeautifulSoup ì‚¬ìš© ê¶Œì¥)
            # ì—¬ê¸°ì„œëŠ” ë°ëª¨ìš©ìœ¼ë¡œ mock ë°ì´í„° ìƒì„±
            mock_jobs = [
                {
                    "title": "AI Ethics Research Scientist",
                    "company": "Anthropic",
                    "location": "San Francisco, CA",
                    "job_type": "industry",
                    "category": "cs",
                    "description": "Research AI safety and alignment with focus on constitutional AI and harmlessness...",
                    "posting_date": datetime.now().strftime("%Y-%m-%d"),
                    "deadline": None,
                    "source_url": "https://www.indeed.com/viewjob?jk=example1",
                    "source_site": "indeed",
                    "tags": ["AI Safety", "Research", "Ethics"]
                },
                {
                    "title": "Technology Policy Manager",
                    "company": "Microsoft",
                    "location": "Washington, DC",
                    "job_type": "industry", 
                    "category": "policy",
                    "description": "Lead policy initiatives around AI governance and responsible deployment...",
                    "posting_date": datetime.now().strftime("%Y-%m-%d"),
                    "deadline": None,
                    "source_url": "https://www.indeed.com/viewjob?jk=example2",
                    "source_site": "indeed",
                    "tags": ["Policy", "AI Governance", "Government Relations"]
                }
            ]
            
            for job in mock_jobs:
                relevance_score = self.calculate_relevance_score(job)
                if relevance_score >= 30:  # ìµœì†Œ ê´€ë ¨ì„± ì„ê³„ì 
                    job['relevance_score'] = relevance_score
                    jobs.append(job)
                    
        except Exception as e:
            print(f"Indeed í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            
        return jobs

    def scrape_academic_jobs_online(self) -> List[Dict]:
        """Academic Jobs Onlineì—ì„œ faculty í¬ì§€ì…˜ í¬ë¡¤ë§"""
        jobs = []
        
        try:
            # ë°ëª¨ìš© mock ë°ì´í„°
            mock_jobs = [
                {
                    "title": "Assistant Professor of AI Ethics",
                    "company": "MIT Computer Science",
                    "location": "Cambridge, MA",
                    "job_type": "faculty",
                    "category": "cs",
                    "description": "Tenure-track position in artificial intelligence with emphasis on ethical AI development...",
                    "posting_date": (datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d"),
                    "deadline": (datetime.now() + timedelta(days=60)).strftime("%Y-%m-%d"),
                    "source_url": "https://academicjobsonline.org/ajo/jobs/example1",
                    "source_site": "academic_jobs_online",
                    "tags": ["Faculty", "Tenure Track", "AI Ethics"]
                },
                {
                    "title": "Professor of Technology Law",
                    "company": "Stanford Law School",
                    "location": "Stanford, CA", 
                    "job_type": "faculty",
                    "category": "law",
                    "description": "Senior faculty position focusing on intersection of technology and law, AI regulation...",
                    "posting_date": (datetime.now() - timedelta(days=5)).strftime("%Y-%m-%d"),
                    "deadline": (datetime.now() + timedelta(days=90)).strftime("%Y-%m-%d"),
                    "source_url": "https://academicjobsonline.org/ajo/jobs/example2",
                    "source_site": "academic_jobs_online",
                    "tags": ["Faculty", "Law", "Technology Policy"]
                }
            ]
            
            for job in mock_jobs:
                relevance_score = self.calculate_relevance_score(job)
                if relevance_score >= 40:  # faculty í¬ì§€ì…˜ì€ ë†’ì€ ê¸°ì¤€
                    job['relevance_score'] = relevance_score
                    jobs.append(job)
                    
        except Exception as e:
            print(f"Academic Jobs Online í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            
        return jobs

    def scrape_idealist_nonprofits(self) -> List[Dict]:
        """Idealistì—ì„œ ë¹„ì˜ë¦¬ ë‹¨ì²´ ì±„ìš©ê³µê³  í¬ë¡¤ë§"""
        jobs = []
        
        try:
            # ë°ëª¨ìš© mock ë°ì´í„°
            mock_jobs = [
                {
                    "title": "AI Policy Advocate",
                    "company": "Electronic Frontier Foundation",
                    "location": "San Francisco, CA",
                    "job_type": "nonprofit",
                    "category": "policy",
                    "description": "Advocate for responsible AI policies and digital rights protection...",
                    "posting_date": (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d"),
                    "deadline": (datetime.now() + timedelta(days=30)).strftime("%Y-%m-%d"),
                    "source_url": "https://www.idealist.org/en/nonprofit-job/example1",
                    "source_site": "idealist",
                    "tags": ["Advocacy", "Digital Rights", "Policy"]
                }
            ]
            
            for job in mock_jobs:
                relevance_score = self.calculate_relevance_score(job)
                if relevance_score >= 35:
                    job['relevance_score'] = relevance_score
                    jobs.append(job)
                    
        except Exception as e:
            print(f"Idealist í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            
        return jobs

    def run_scraping(self) -> Dict[str, Any]:
        """ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("ğŸ” ì±„ìš©ê³µê³  í¬ë¡¤ë§ ì‹œì‘...")
        
        # ê° ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§
        indeed_jobs = self.scrape_indeed()
        academic_jobs = self.scrape_academic_jobs_online()
        nonprofit_jobs = self.scrape_idealist_nonprofits()
        
        # ëª¨ë“  ì±„ìš©ê³µê³  í•©ì¹˜ê¸°
        all_jobs = indeed_jobs + academic_jobs + nonprofit_jobs
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_jobs = []
        for job in all_jobs:
            if job['source_url'] not in seen_urls:
                seen_urls.add(job['source_url'])
                unique_jobs.append(job)
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        unique_jobs.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # í†µê³„ ê³„ì‚°
        stats = self.calculate_stats(unique_jobs)
        
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(unique_jobs)}ê°œ ì±„ìš©ê³µê³  ìˆ˜ì§‘")
        
        return {
            "jobs": unique_jobs,
            "stats": stats,
            "last_update": datetime.now().isoformat(),
            "total_scraped": len(all_jobs),
            "duplicates_removed": len(all_jobs) - len(unique_jobs)
        }

    def calculate_stats(self, jobs: List[Dict]) -> Dict[str, int]:
        """ì±„ìš©ê³µê³  í†µê³„ ê³„ì‚°"""
        stats = {
            "total": len(jobs),
            "faculty": len([j for j in jobs if j['job_type'] == 'faculty']),
            "industry": len([j for j in jobs if j['job_type'] == 'industry']),
            "nonprofit": len([j for j in jobs if j['job_type'] == 'nonprofit']),
            "new_today": len([j for j in jobs if j['posting_date'] == datetime.now().strftime("%Y-%m-%d")]),
            "by_category": {
                "law": len([j for j in jobs if j['category'] == 'law']),
                "policy": len([j for j in jobs if j['category'] == 'policy']),
                "cs": len([j for j in jobs if j['category'] == 'cs']),
                "ischool": len([j for j in jobs if j['category'] == 'ischool'])
            }
        }
        return stats

    def save_to_json(self, data: Dict[str, Any]):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        # data ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('data', exist_ok=True)
        
        # ë©”ì¸ ë°ì´í„° íŒŒì¼
        with open('data/jobs.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # í†µê³„ë§Œ ë³„ë„ íŒŒì¼ë¡œë„ ì €ì¥
        with open('data/stats.json', 'w', encoding='utf-8') as f:
            json.dump(data['stats'], f, ensure_ascii=False, indent=2)
        
        # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„
        with open('data/last_update.json', 'w', encoding='utf-8') as f:
            json.dump({
                "last_update": data['last_update'],
                "total_jobs": data['stats']['total']
            }, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: data/jobs.json ({data['stats']['total']}ê°œ ì±„ìš©ê³µê³ )")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = AIJobScraper()
    
    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        data = scraper.run_scraping()
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        scraper.save_to_json(data)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
        print(f"   ì´ ì±„ìš©ê³µê³ : {data['stats']['total']}ê°œ")
        print(f"   Faculty: {data['stats']['faculty']}ê°œ")
        print(f"   Industry: {data['stats']['industry']}ê°œ") 
        print(f"   Non-profit: {data['stats']['nonprofit']}ê°œ")
        print(f"   ì˜¤ëŠ˜ ì‹ ê·œ: {data['stats']['new_today']}ê°œ")
        print(f"   ì¤‘ë³µ ì œê±°: {data['duplicates_removed']}ê°œ")
        
        print("\nğŸ¯ ì¹´í…Œê³ ë¦¬ë³„:")
        for category, count in data['stats']['by_category'].items():
            print(f"   {category}: {count}ê°œ")
            
        print(f"\nğŸ•’ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {data['last_update']}")
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())